---
title: "DS Project in R"
author: "Robiatul Adawiyah Al-Qosh"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    toc: true
    toc_depth: 4
    toc_float: true
    theme: flatly
    highlight: haddock
  pdf_document:
    toc: true
  word_document:
    toc: true
---

This is the Data Science Project created by Robiatul Al-Qosh.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The library used in this task mainly comes from *tidyverse* package.

```{r}
library(tidyverse)
```

## Task A

This task mainly about web scraping that referring to moodle material in week 3-4.

### Task A1

In this task, I use _rvest_ package for web scraping.

```{r}
#install.packages("rvest")
library(rvest)

#Assign the website
site <- "https://en.wikipedia.org/wiki/ICC_Men%27s_T20I_Team_Rankings"
```

Then, read the website using read_html command and extract that into a table using html_table command.

```{r}
raw_html <- read_html(site)
tables <-  html_table(raw_html, fill = TRUE)
```

Historical rankings table has six columns, which is the table 8.

```{r}
hist_rank <- tables[[8]]
#hist_rank
```

I need to delete the last data, since it contains the last updated information.

```{r}
hist_rank <- hist_rank[-27, ]
#hist_rank
```

Remove the citation in the date columns and change the "Present" data into the last updated date, so I can convert the chr data into date data.

```{r}
hist_rank <- hist_rank %>%
  mutate(Start = gsub("\\[.*?\\]", "", Start),
         End = gsub("\\[.*?\\]", "", End),
         End = gsub("Present", "08 July 2024", End))
#hist_rank
```

Convert Start and End tables to date data type.

```{r}
hist_rank$Start <- as.Date(hist_rank$Start,  format = "%d %B %Y")
hist_rank$End <- as.Date(hist_rank$End,  format = "%d %B %Y")
#hist_rank
```

Then, remove the "days" or "day" in Duration and assign the column into integer data type. 

```{r}
hist_rank <- hist_rank %>%
  mutate(Duration = gsub("[ days, day]", "", Duration),
         Duration = as.integer(Duration))

#hist_rank
```

Finally, I can summarise the table and sort it in descending manner based on average duration.

```{r}
hist_rank <- hist_rank %>%
  group_by(Country) %>%
  summarise(Earliest_start = min(Start),
            Latest_end = max(End),
            Average_duration = round(mean(Duration), 2)) %>%
  arrange(desc(Average_duration))

hist_rank
```

### Task A2

In this task, I am interested to know the number of houses in the Westeros. The Westeros it self is a nation of a fantasy world that is created by George R.R. Martin in his book series "A Song of Ice and Fire". So, I extract the table from following link and select the 2 important columns, which are Region name and House name.

```{r}
site2 <- "https://awoiaf.westeros.org/index.php/List_of_Houses"
raw_html2 <- read_html(site2)
tables2 <-  html_table(raw_html2, fill = TRUE)

westeros_houses <- tables2[[1]] %>%
  select(Region, House)

#westeros_houses
```

Some data in Region have unnecessary additional information in brackets, such as the name of the island. Consequently, I need to remove them in order to make the clear classification. Moreover, since some houses are located in more than one region that is identified by ";" or "/", I have to separated them into another rows.

```{r}
westeros_houses <- westeros_houses %>%
  separate_rows(Region, sep = "\\;") %>%
  separate_rows(Region, sep = "\\/") %>%
  mutate(Region = gsub("\\(.*?\\)", "", Region),
         Region = str_squish(Region))
  

#westeros_houses
```

Then, I calculate the number of houses for each region using summarise command.

```{r}
houses_per_region <- westeros_houses %>%
  group_by(Region) %>%
  summarise(Number_of_houses = n())

houses_per_region
```

Finally, I make the bar chart to plot the number of houses for each region in Westeros.

```{r}
ggplot(houses_per_region,
       aes(x = reorder(Region, -Number_of_houses), y = Number_of_houses)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(title = "Number of Houses for Each Region in Westeros",
       x = "Region Name", y = "Number of Houses") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

I add the map of the Westeros to give the clear perspective about the regions.

<img src="https://atlasoficeandfireblog.wordpress.com/wp-content/uploads/2016/03/westeros-regions-named.png" alt="Westeros Regions" style="width:30%; height:auto;"/>

Although the area of the North region is the biggest, the total houses in the Reach has the largest number following by the North itself. While the Dorne has the smallest number of houses in the Westeros. In addition, there are houses that have unknown regions. It can be interpreted that these houses are uncharted by George R.R. Martin in his books. If in the future book he mentions these houses and regions, the chart might be different.

## Task B

First of all, I load the "Olympics_tweets.csv" file.

```{r}
olympics_tweets <- read.csv("Olympics_tweets.csv")

#olympics_tweets
```

Summary of the data

```{r}
summary(olympics_tweets)
```

### 1. The Creators of The Tweets Analysis

#### Answer 1.1

First, I need to change the user_created_at column data type into date.

```{r}
olympics_tweets$user_created_at <- as.Date(olympics_tweets$user_created_at,
                                           format="%d/%m/%Y %H:%M")
```

Then, I create a necessary table that contains the user_created_at in year form and the number of user by counting the id distinctly.

```{r}
user_per_year <- olympics_tweets %>%
  group_by(year = year(user_created_at)) %>%
  summarise(number_of_users = n_distinct(user_screen_name))
```

Finally, create the bar chart.

```{r}
ggplot(na.omit(user_per_year),
       aes(x = year, y = number_of_users)) +
  geom_bar(stat = "identity", fill = "steelblue4") +
  labs(title = "Number of Users Created Who Tweeted about Tokyo Olympic across the Years",
       x = "Year", y = "Number of Users") +
  theme_minimal()
  
```

#### Answer 1.2

Users after 2010 is got by filtering the year that larger than 2010. And since the user_screen_name might have more than one tweet, I add distinct command to avoid the repetition.

```{r}
user_after2010 <- olympics_tweets %>%
  filter(year(user_created_at) > 2010) %>%
  group_by(user_screen_name) %>%
  select(user_screen_name, user_created_at, user_followers) %>%
  distinct()
```

After which, I create new table that contains the year and the average followers.

```{r}
folls_user_after2010 <- user_after2010 %>%
  group_by(year = year(user_created_at)) %>%
  summarise(avg_user_followers = round(mean(user_followers)))
```

Eventually, the bar chart can be created.

```{r}
ggplot(folls_user_after2010,
       aes(x = year, y = avg_user_followers)) +
  geom_bar(stat = "identity", fill = "lightblue3") +
  labs(title = "2011-2021 Users' Average Followers Who Tweeted about Tokyo Olympic",
       x = "Year", y = "Users' Average Followers") +
  theme_minimal()
```

#### Answer 1.3

From the 1.1 bar chart, it can be seen that users created in 2009 are the ones who tweeted the most about the Tokyo Olympics. This could be related to their age and interest in the Olympics during their golden years. Since there is a possibility that users created at 2009 are in their 20s-30s if they created their accounts when they were teenagers. Meanwhile, the increase in the number of users created in 2020 and 2021 who tweeted about the Olympics might be because they created accounts to celebrate and enjoy the euphoria of the Olympics on Twitter. 

On the other hand, the 1.2 bar chart shows that 2011 users have the largest number of followers while users in the following years experienced a gradual decrease in the number of followers.This can be interpreted as the newer the account, the fewer the number of followers it has, since the earliest users have the most followers then the latest one.

#### Answer 1.4

First, since there are inconsistency in user_location values, I would try to equalize abbreviation of the location and its actual name. However, I find the odd values such as "she/her", this value should be categorized as gender not location. So, I decide to remove them. Then, I calculate many variations of locations they put on their twitter account with user_location, and sort it on descending manner based on the frequency. Therefore, this is the top 10 most frequent locations.

```{r}
user_per_loc <- olympics_tweets %>%
  mutate(user_location = gsub("\\- .*", "", user_location),
         user_location = gsub("\\-.*", "", user_location),
         user_location = gsub("UK", "United Kingdom", user_location),
         user_location = gsub("CA", "California", user_location),
         user_location = gsub("TX", "Texas", user_location),
         user_location = gsub("NY", "New York", user_location),
         user_location = gsub("LA", "Los Angeles", user_location),
         user_location = gsub("US|United States|USA", "USA", user_location)) %>%
  filter(!grepl("he|him|she|her|they|them", tolower(user_location))) %>%
  group_by(user_location) %>%
  summarise(number_of_users = n_distinct(user_screen_name)) %>%
  arrange(desc(number_of_users)) %>%
  na.omit() %>%
  head(10)
user_per_loc
```

I realize that the locations still not consistent enough due to the combination of country, city, etc. However, I can not generalize the location into cities or countries because of the lack of the information and data that users pun in their account. Therefore, I let the top 10 locations contain country or city like the table above. Meanwhile, for the tweets which have both country and city, I still count it as one tweet and consider it as city. This can be done with removing the country of those tweets.

```{r}
user_tweets <- olympics_tweets %>%
  mutate(user_location = gsub("\\- .*", "", user_location),
         user_location = gsub("\\-.*", "", user_location),
         user_location = gsub("UK", "United Kingdom", user_location),
         user_location = gsub("CA", "California", user_location),
         user_location = gsub("TX", "Texas", user_location),
         user_location = gsub("NY", "New York", user_location),
         user_location = gsub("LA", "Los Angeles", user_location),
         user_location = gsub("US|United States|USA", "USA", user_location)) %>%
  filter(user_location %in% user_per_loc$user_location) %>%
  group_by(user_location) %>%
  summarise(number_of_tweets = n())
user_tweets
```

To see  many tweets are associated with these top 10 most frequent location values clearly, I join these two table that already created.

```{r}
tweets_loc <- left_join(user_per_loc, user_tweets,
                        by = "user_location")
tweets_loc
```

From the table, it can be seen that the number of tweets is bigger than the number of users in one location. This can be interpreted as a person might posted more than one tweet.

### 2. The Tweets Analysis

#### Answer 2.1

Since the date column still in chr date type, I need to change it into date data type.

```{r}
olympics_tweets$date <- as.Date(olympics_tweets$date,
                                format="%d/%m/%Y %H:%M")
```

After the date column fixed, I created summarise table that contains the date and the number of tweets. 

```{r}
tweets_per_day <- olympics_tweets %>%
  group_by(date) %>%
  summarise(number_of_tweets = n()) %>%
  na.omit()
tweets_per_day
```

Then, bar chart can be created.

```{r}
ggplot(tweets_per_day,
       aes(x = date, y = number_of_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Number of Tweets per Day",
       x = "Date", y = "Number of Tweets") +
  theme_minimal()
```

The bar chart shows that 24 July 2021 is the date that has the lowest number of tweets.

#### Answer 2.2

First of all, I count the character of every text, and then categorize them into 7 required definition. After that, I summarise them based on the category.

```{r}
tweets_char <- olympics_tweets %>%
  mutate(text = nchar(text),
         length_category = case_when(
           text >= 1 & text <= 40 ~ "[1, 40]",
           text >= 41 & text <= 80 ~ "[41, 80]",
           text >= 81 & text <= 120 ~ "[81, 120]",
           text >= 121 & text <= 160 ~ "[121, 160]",
           text >= 161 & text <= 200 ~ "[161, 200]",
           text >= 201 & text <= 240 ~ "[201, 240]",
           text >= 241 ~ ">= 241"),
         length_category = factor(length_category, levels = c("[1, 40]",
                                                              "[41, 80]",
                                                              "[81, 120]",
                                                              "[121, 160]",
                                                              "[161, 200]",
                                                              "[201, 240]",
                                                              ">= 241"))) %>%
  group_by(length_category) %>%
  summarise(number_of_tweets = n())
tweets_char
```

Finally, I can create the bar chart from the previous table.

```{r}
ggplot(tweets_char,
       aes(x = length_category, y = number_of_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue2") +
  labs(title = "Number of Tweets Based on The Length of Tweets' Characters",
       x = "Length Category", y = "Number of Tweets") +
  theme_minimal()
```

#### Answer 2.3

I create the new table that contains the tweets that mention another account and how many accounts the tweet mention.

```{r}
tweets_tag <- olympics_tweets %>%
  filter(grepl(" @.", text)) %>%
  mutate(number_of_ats = str_count(text, "@")) %>%
  select(number_of_ats, text)
```

Then count it based on the number of rows.

```{r}
print(paste("The number of tweets that contain another account is ",
            nrow(tweets_tag),
            " tweets"))
```

On the other hand, I filter the number_of_ats that mention more than 2 account and also count the number of rows.

```{r}
print(paste("The number of  tweets that contain at least 3 another account is ",
            nrow(tweets_tag %>% filter(number_of_ats >= 3)),
            " tweets"))
```

#### Answer 2.4

To get the top 20 most frequent terms, I change the text column into a lowercase form, remove the special character, and trim the text. Then count it based on descending manner.

```{r}
tweets_words <- olympics_tweets %>%
  mutate(text = tolower(text),
         text = gsub("[£$€@#¦:;.-_™]", "", text),
         text = str_squish(text)) %>%
  separate_rows(text, sep = "\\ ") %>%
  group_by(text) %>%
  summarise(frequency = n(), .groups = "drop") %>%
  arrange(desc(frequency)) %>%
  select(text, frequency) %>%
  head(20)
tweets_words
```

From the tweets_words table, it can be seen that the top 20 most frequent terms have so many stopwords. So I create the another table that remove stopwords among the terms.

```{r}
tweets_wo_stopwords <- olympics_tweets %>%
  mutate(text = tolower(text),
         text = str_remove_all(text, "[[:punct:][:digit:]]"),
         text = str_remove_all(text, "\\b\\w{1,2}\\b"),
         text = str_remove_all(text, "\\b(the|for|and|but|not|are|was|who|when|why|itâ|
         |has|had|she|they|his|her|him|were|with|from|have|will|into|over|your|their|amp|
         |its|you|may|all|this|that|just|what|about|like|how|after|can|been|)\\b"),
         text = gsub("[£$€@¦™]", "", text),
         text = str_squish(text)) %>%
  separate_rows(text, sep = "\\ ") %>%
  group_by(text) %>%
  summarise(frequency = n(), .groups = "drop") %>%
  arrange(desc(frequency)) %>%
  select(text, frequency) %>%
  head(20)
tweets_wo_stopwords
```

## Task C

Read the utterances train, test, and validation files and switch the column name into the simple one.

```{r}
utterances_train <- read.csv("dialogue_utterance_train.csv")
utterances_train <- utterances_train %>%
  rename(Dialogue_ID = Dialogue_ID..Annonymised.,
         Interlocutor = Interlocutor..either.Chatbot.or.Student.)

utterances_test <- read.csv("dialogue_utterance_test.csv")
utterances_test <- utterances_test %>%
  rename(Dialogue_ID = Dialogue_ID..Annonymised.,
         Interlocutor = Interlocutor..either.Chatbot.or.Student.)

utterances_validation <- read.csv("dialogue_utterance_validation.csv")
utterances_validation <- utterances_validation %>%
  rename(Dialogue_ID = Dialogue_ID..Annonymised.,
         Interlocutor = Interlocutor..either.Chatbot.or.Student.)
```

Read the usefulness train, test, and validation files.

```{r}
usefulness_train <- read.csv("dialogue_usefulness_train.csv")

usefulness_test <- read.csv("dialogue_usefulness_test.csv")

usefulness_test_ans <- read.csv("dialogue_usefulness_test-Answer.csv")

usefulness_validation <- read.csv("dialogue_usefulness_validation.csv")
```

### Task C1

First, I need to merge the utterances and usefulness train data.

```{r}
train_data <- merge(utterances_train,
                    usefulness_train,
                    by = "Dialogue_ID")
#train_data
```

The features I can use are the number of utterances, dialogue duration, average utterance length, and sentiment score. These four things might be useful to empower the training of machine learning models.

Feature one is the number of utterances. I make usefulness score category that represent unuseful and useful and count the number of utterances per dialogue based on the row. In addition, at the end I remove the neutral score category to make the clear difference.

```{r}
feature1_nou <- train_data %>%
  mutate(score_category = case_when(Usefulness_score > 3 ~ "4 - 5",
                                    Usefulness_score < 3 ~ "1 - 2",
                                    Usefulness_score == 3 ~ "3"),) %>%
  group_by(Dialogue_ID, score_category) %>%
  summarise(number_of_utterances = n(),
            from_student = sum(Interlocutor == "Student"),
            from_chatbot = sum(Interlocutor == "Chatbot"),
            .groups = "drop") %>%
  filter(score_category != "3")
```

Feature two is the dialogue duration. basically, the code is similar with the number of utterances but I add the timestamp execution to change the data type, also the summarise is contain the min max timestamp and the substraction between them to find the duration.

```{r}
feature2_dd <- train_data %>%
  mutate(score_category = case_when(Usefulness_score > 3 ~ "4 - 5",
                                    Usefulness_score < 3 ~ "1 - 2",
                                    Usefulness_score == 3 ~ "3"),
         Timestamp = as.POSIXct(Timestamp, format = "%Y-%m-%d %H:%M:%S")) %>%
  group_by(Dialogue_ID, score_category) %>%
  summarise(start_time = min(Timestamp),
            end_time = max(Timestamp),
            duration = as.numeric(difftime(end_time, start_time, units = "mins")),
            .groups = "drop") %>%
  filter(score_category != "3")
```

The boxplot of the number of utterances based on usefulness score category.

```{r}
ggplot(feature1_nou,
       aes(x = score_category, y = number_of_utterances)) +
  geom_boxplot(color = "steelblue3") +
  labs(title = "Boxplot Number of Utterances of Usefulness Score Category",
       x = "Usefulness Score Category",
       y = "Number of Utterances") +
  theme_minimal()
```

The boxplot of the number of utterances based on dialogue duration.

```{r}
ggplot(feature2_dd,
       aes(x = score_category, y = duration)) +
  geom_boxplot(color = "steelblue3") +
  labs(title = "Boxplot Duration of Usefulness Score Category",
       x = "Usefulness Score Category",
       y = "Duration") +
  theme_minimal()
```

From both number of utterances and dialogue duration boxplots, it can be seen that "useful" reviews tend to have a greater number of range utterances and longer dialogue durations. 

```{r}
t.test(feature1_nou$number_of_utterances, feature2_dd$duration)
```

However, the dialogue duration feature seems to have higher statistical significance in average compared to the number of utterances.

### Task C2

First, I create the table that contains all the features which are the number of utterances, dialogue duration, average utterance length, and sentiment score.

```{r}
#install.packages("tidytext")
library(tidytext)

features_train_data <- train_data %>%
  group_by(Dialogue_ID, Usefulness_score) %>%
  summarise(number_of_utterances = n(),
            duration = as.numeric(difftime(max(Timestamp),
                                           min(Timestamp),
                                           units = "mins")),
            avg_utterances_length = mean(nchar(Utterance_text)),
            .groups = "drop") %>%
  #Creating sentiment analysis score in different part, then merged them
  left_join(train_data %>%
              ungroup() %>%
              unnest_tokens(word, Utterance_text) %>%
              inner_join(get_sentiments("bing"), by = "word") %>%
              #Change the sentiment into numbers
              mutate(score = case_when(sentiment == "positive" ~ 1,
                                       sentiment == "negative" ~ -1)) %>%
              group_by(Dialogue_ID) %>%
              summarise(sentiment_score = sum(score, na.rm = TRUE),
                        .groups = "drop"))
```

Then, I build the regression tree model (from Applied Class 7 in the moodle).

```{r}
#install.packages("rpart")
library(rpart)

model1 <- rpart(Usefulness_score ~ ., 
               data = features_train_data, 
               method = "anova")
```

Afterwhich, I create the validation_data table from utterances_validation and usefulness_validation, and from it, another features table created but for validation.

```{r}
validation_data <- merge(utterances_validation,
                         usefulness_validation,
                         by = "Dialogue_ID")

features_validation_data <- validation_data %>%
  group_by(Dialogue_ID, Usefulness_score) %>%
  summarise(number_of_utterances = n(),
            duration = as.numeric(difftime(max(Timestamp),
                                           min(Timestamp),
                                           units = "mins")),
            avg_utterances_length = mean(nchar(Utterance_text)),
            .groups = "drop") %>%
  left_join(validation_data %>%
              ungroup() %>%
              unnest_tokens(word, Utterance_text) %>%
              inner_join(get_sentiments("bing"), by = "word") %>%
              mutate(score = case_when(sentiment == "positive" ~ 1,
                                       sentiment == "negative" ~ -1)) %>%
              group_by(Dialogue_ID) %>%
              summarise(sentiment_score = sum(score, na.rm = TRUE),
                        .groups = "drop"))
```

After the model1 and the data for validation created, I apply the model to predict the Usefulness_score in features_validation_data, then count the RMSE of the prediction and actual data.

```{r}
predictions <- predict(model1, newdata = features_validation_data)

#Counting the RMSE
rmse <- sqrt(mean((features_validation_data$Usefulness_score - predictions)^2))
print(rmse)

```

### Task C3

To improve the model1, I will try to select features that might be the important one, which are  number_of_utterances and sentiment_score. Before, I already examined another figures, but the best rmse result is shown by number_of_utterances and sentiment_score. Moreover, I think it will be better if I remove the null values from the data.

```{r}
model2 <- rpart(Usefulness_score ~ number_of_utterances + sentiment_score,
                data = na.omit(features_train_data),
                method = "anova")

predictions2 <- predict(model2, newdata = na.omit(features_validation_data))

#Counting the RMSE
rmse2 <- sqrt(mean((na.omit(features_validation_data)$Usefulness_score - predictions2)^2))
print(rmse2)
```

In addition, I also try to build another model which is random forest. Nevertheless, the result is not better than the previous model. So, I think I will go with the regression tree model.

```{r}
#install.packages("randomForest")
library(randomForest)

model3 <- randomForest(Usefulness_score ~ .,
                       data = features_train_data)

predictions3 <- predict(model3, newdata = na.omit(features_validation_data))

#Counting the RMSE
rmse3 <- sqrt(mean((na.omit(features_validation_data)$Usefulness_score - predictions3)^2))
rmse3
```

### Task C4

For example, my dialogue id is 1434, so I will try to get the dialogue test from the utterances_validation file.

```{r}
trial <- utterances_validation %>%
  filter(Dialogue_ID == "1434")
trial
```

So, before I apply the model, I need to prepare the features first.

```{r}
features_trial <- trial %>%
  group_by(Dialogue_ID) %>%
  summarise(number_of_utterances = n(),
            duration = as.numeric(difftime(max(Timestamp),
                                           min(Timestamp),
                                           units = "mins")),
            avg_utterances_length = mean(nchar(Utterance_text)),
            .groups = "drop") %>%
  left_join(trial %>%
              ungroup() %>%
              unnest_tokens(word, Utterance_text) %>%
              inner_join(get_sentiments("bing"), by = "word") %>%
              mutate(score = case_when(sentiment == "positive" ~ 1,
                                       sentiment == "negative" ~ -1)) %>%
              group_by(Dialogue_ID) %>%
              summarise(sentiment_score = sum(score, na.rm = TRUE),
                        .groups = "drop"))
features_trial
```

Then, lets predict the Usefulness_score by applying the model2 which is I think it is the best model.

```{r}
predict_trial <- predict(model2, newdata = features_trial)
predict_trial
```

Afterwhich, lets see the groundtruth value of dialogue id 1434 in usefulness_validation.

```{r}
groundtruth <- usefulness_validation %>%
  filter(Dialogue_ID == "1434")
groundtruth
```

In my opinion, the prediction and groundtruth usefulness score is close enough (if I round the result of the prediction, it will be exactly same). This is might be because of the number of utterances and sentiment score features that used in building model2.

### Task C5

First I need the make the utterances_test file contains all the features.

```{r}
features_test <- utterances_test %>%
  group_by(Dialogue_ID) %>%
  summarise(number_of_utterances = n(),
            duration = as.numeric(difftime(max(Timestamp),
                                           min(Timestamp),
                                           units = "mins")),
            avg_utterances_length = mean(nchar(Utterance_text)),
            .groups = "drop") %>%
  left_join(utterances_test %>%
              ungroup() %>%
              unnest_tokens(word, Utterance_text) %>%
              inner_join(get_sentiments("bing"), by = "word") %>%
              mutate(score = case_when(sentiment == "positive" ~ 1,
                                       sentiment == "negative" ~ -1)) %>%
              group_by(Dialogue_ID) %>%
              summarise(sentiment_score = sum(score, na.rm = TRUE),
                        .groups = "drop"))
features_test
```

Then, apply the model to predict the Usefulness_score.

```{r}
predict_test <- predict(model2, newdata = features_test)
predict_test
```

Se, I input the result into a table with dialogue id. Moreover, since the Usefulness_score is a round value from 1 - 5, I think it will be better to round the score prediction.

```{r}
dialogue_usefulness_test <- features_test %>%
  mutate(Usefulness_score_prediction = round(predict_test)) %>%
  select(Dialogue_ID, Usefulness_score_prediction)
dialogue_usefulness_test
```

Finally, I save the table into csv file with required name.

```{r}
write.csv(dialogue_usefulness_test, "Al-Qosh_34269193_dialogue_usefulness_test.csv", row.names = FALSE)
```

