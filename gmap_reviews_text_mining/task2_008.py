# -*- coding: utf-8 -*-
"""task2_008.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13QqGUvJ9FgIYBnDX76lK7KuYPf-Mz477

<div class="alert alert-block alert-danger">

# FIT5196 Task 2 in Assessment 1
#### Student Name: Robiatul Adawiyah Al-Qosh
#### Student ID: 34269193

Date: 20/10/2024


Environment: Python 3.10

Libraries used:
* os (for interacting with the operating system, included in Python xxxx)
* pandas 1.1.0 (for dataframe, installed and imported)
* json (to handle json file)
* langid (for handling languages)
* re (to define and use regular expressions)
* nltk 3.5 (Natural Language Toolkit, installed and imported)
* nltk.tokenize (for tokenization, installed and imported)
* nltk.corpus (to handle stopwords)
* nltk.stem (for stemming the tokens, installed and imported)
* nltk.util (to handle bigrams)
* itertools (for performing operations on iterables)
* collections (to handle different object)
* math (for mathematic calculation)
* uuid (to uniquely identify information in systems)

    </div>

<div class="alert alert-block alert-info">
    
## Table of Contents

</div>

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Input File](#examine) <br>
[4. Loading and Parsing Files](#load) <br>
$\;\;\;\;$[4.1. Tokenization](#tokenize) <br>
$\;\;\;\;$[4.2. Whatever else](#whetev) <br>
$\;\;\;\;$[4.3. Genegrate numerical representation](#whetev1) <br>
[5. Writing Output Files](#write) <br>
$\;\;\;\;$[5.1. Vocabulary List](#write-vocab) <br>
$\;\;\;\;$[5.2. Sparse Matrix](#write-sparseMat) <br>
[6. Summary](#summary) <br>
[7. References](#Ref) <br>

<div class="alert alert-block alert-success">
    
## 1.  Introduction  <a class="anchor" name="Intro"></a>

This assessment concerns textual data and the aim is to extract data, process them, and transform them into a proper format. The dataset provided is in the format of a PDF file containing ....

-------------------------------------

<div class="alert alert-block alert-success">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>

In this assessment, any python packages is permitted to be used. The following packages were used to accomplish the related tasks:

* **os:** to interact with the operating system, e.g. navigate through folders to read files
* **pandas:** to work with dataframes
* **json:** to handle json file
* **langid:** to choose english language only from reviews
* **re:** to define and use regular expressions
* **nltk:** to use Natural Language Toolkit, installed and imported
* **RegexpTokenizer:** to tokenize reviews
* **stopwords:** to handle stopwords
* **PorterStemmer:** to do stemming
* **ngrams:** to handle bigrams
* **chain:** to overcome nasted list
* **Counter:** to count frequency of unigrams
* **defaultdict:** handle dictionary
* **log:** for PMI calculation
* **uuid:** to extract gmap_id for sparse matrix
"""

import os
import pandas as pd
import json
import langid
import re
import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk import FreqDist
from nltk.stem import PorterStemmer
from nltk.util import ngrams
from itertools import chain
from collections import Counter
from collections import defaultdict
from math import log
import uuid

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 3.  Examining Input File <a class="anchor" name="examine"></a>
"""

from google.colab import drive
drive.mount('/content/drive')

"""Set working directory."""

student_008 = "/content/drive/MyDrive/S2 Monash University/MUI Y2 T1/ITI5196 Data Wrangling/Assignment 1/student_008"
# Please adjust the path

"""Let's check what the file contains. For this purpose, loading the csv file is done first to find out the gmap_id which has at least 70 text reviews.


"""

# Read csv file from working directory
csv_data = pd.read_csv(os.path.join(student_008, "task1_008.csv"))
csv_data

# Filter gmap_id that have at least 70 reviews
filtered_gmap_ids = csv_data[csv_data['review_text_count'] >= 70]['gmap_id'].tolist()

# Shows the gmap_id
print(f"The number of qualified gmap_id is: ", len(filtered_gmap_ids))
print(filtered_gmap_ids)

"""It is noticed that the csv file contains approximately 71 qualified gmap_ids.

-------------------------------------

<div class="alert alert-block alert-success">
    
## 4.  Loading and Parsing File <a class="anchor" name="load"></a>

-------------------------------------

<div class="alert alert-block alert-warning">
    
### 4.1. Text Extraction <a class="anchor" name="tokenize"></a>

In this section, the JSON file will be opened to extract the review text.
"""

# Open JSON file form working directory
with open(os.path.join(student_008, "task1_008.json"), 'r') as f:
    json_data = json.load(f)

# Extract review text from JSON file based on filterd gmap_id
reviews = []
for business in json_data:
    if business['gmap_id'] in filtered_gmap_ids:
        for review in business['reviews']:
            # Take review taxt that not "None" and in english only
            if 'review_text' in review and review['review_text'] != "None" and langid.classify(review['review_text'])[0] == 'en':
                review_text = review['review_text']

                reviews.append(review_text)

"""Let's examine the dictionary generated. For counting the total number of reviews extracted, the len commad can be used."""

print(f"The number of reviews extracted is: ", len(reviews))

"""It is noticed that the number of reviews extracted is less than the number of reviews of gmap_id qualified. This might be due to eliminating non-English reviews.

-------------------------------------

<div class="alert alert-block alert-warning">
    
### 4.2. Tokenization <a class="anchor" name="tokenize"></a>

Tokenization is a principal step in text processing and producing unigrams. In this section, the reviews will be tokenized first.
"""

# Create tokenization that follows required regex
tokenizer = RegexpTokenizer(r"[a-zA-Z]+")

# Create function tokenize
def tokenize(text):
    return tokenizer.tokenize(text.lower())

# Apply the function into review text
unigram_tokens = [tokenize(review) for review in reviews]

print(unigram_tokens)

"""Remove common words (context-independent stopwords) that are not necessary for the analysis."""

# Get stopwords set from nltk
nltk.download('stopwords')
stopwords_en = set(stopwords.words('english'))
print(stopwords_en)

# Read the stopwords_en.txt file provided
with open(os.path.join(student_008, "stopwords_en.txt"), 'r') as f:
    stopwords_txt = set(f.read().splitlines())

# Adding the stopwords form nltk and txt file
stopwords_en.update(stopwords_txt)

print(stopwords_en)

# Function to remove context-independent stopwords
def remove_stopwords_ci(tokens):
    return [token for token in tokens if token not in stopwords_en]

# Apply the function into combined_tokens
tokens_no_stopwords1 = [remove_stopwords_ci(tokens) for tokens in unigram_tokens]

print(tokens_no_stopwords1)

"""Remove common words (context-dependent stopwords) that are appear more than 95% in the business."""

# Function to find the word that used in 95% of the business
def stopwords_cd(tokens_list, threshold=0.95):
    freq_dist = FreqDist(chain.from_iterable(tokens_list))
    num_businesses = len(tokens_list)
    return set(token for token, count in freq_dist.items() if count / num_businesses > threshold)

# Find context-dependent stopwords from the combined tokens
cd_stopwords = stopwords_cd(unigram_tokens)

print(cd_stopwords)

# Function to delete context-dependent stopwords
def remove_stopwords_cd(tokens):
    return [token for token in tokens if token not in cd_stopwords]

# Apply the function into tokens_no_stopwords1
tokens_no_stopwords2 = [remove_stopwords_cd(tokens) for tokens in tokens_no_stopwords1]

print(tokens_no_stopwords2)

"""At this point, removing context-dependent stopwords may not be necessary because they may already be included in context-independent stopwords.

Then, remove common words that are appear more than 50% in the business.
"""

# Function to find the word that used in 50% of the business
def find_common_words(tokens_list, threshold=0.5):
    freq_dist = FreqDist(chain.from_iterable(tokens_list))
    num_businesses = len(tokens_list)
    return set(token for token, count in freq_dist.items() if count / num_businesses > threshold)

# Find common words from the combined tokens
common_words = find_common_words(unigram_tokens)

print(common_words)

# Function to delete common words
def remove_common_words(tokens):
    return [token for token in tokens if token not in common_words]

# Apply the function into stemmed
rare_tokens = [remove_common_words(tokens) for tokens in tokens_no_stopwords2]

print(rare_tokens)

"""Well, removing common words that appear 50% in business reviews may not be necessary too because they may already removed in the previous step.

Afterwhich, make sure that tokens that are too short (less than 3 characters) are removed from the list.
"""

# Remove the words that less than 3 characters
cleaned_tokens = [[token for token in tokens if len(token) >= 3] for tokens in rare_tokens]

print(cleaned_tokens)

"""Next, stemming using Porter's algorithm is done to reduce each token to its basic form."""

# Stemmer inisialization
stemmer = PorterStemmer()

# Function to steam each tokens
def apply_stemming(tokens):
    return [stemmer.stem(token) for token in tokens]

# Apply the function into tokens_no_stopwords2
stemmed_tokens = [apply_stemming(tokens) for tokens in cleaned_tokens]

print(stemmed_tokens)

"""Eventually, the cleaned unigram tokens are put into a list and the frequency of each word is calculated."""

all_unigram_tokens = list(chain.from_iterable(stemmed_tokens))

freq_unigram = Counter(all_unigram_tokens)

print(freq_unigram)

"""On the other hand, a bigram combination is created from the tokens that already cleaned but not stemmed. This step generates bigrams (two consecutive words) from the tokens generated from the tokenization process."""

# Create function bigrams
def get_bigrams(tokens):
    return list(ngrams(tokens, 2))

# Apply the function into unigram_toke
bigrams_tokens = [get_bigrams(tokens) for tokens in cleaned_tokens]
print(bigrams_tokens)

"""Then, calculate the frequency of the bigram tokens."""

# Combine all bigrams into one list and make sure the data type is a tuple.
flattened_bigrams = [bigram for review in bigrams_tokens for bigram in review]

print(flattened_bigrams)

# Create FreqDist based on bigram
freq_bigrams = FreqDist(flattened_bigrams)

freq_bigrams

"""Make a PMI calculation then take the top 200 bigrams based on that calculation."""

# N is nexessary to calculate PMI
N = sum(freq_unigram.values())

# Create a function to calculate PMI for every bigram
def calculate_pmi(bigram, freq, freq_unigram, N):
    w1, w2 = bigram  # Unpack bigram

    # Make sure frequency is not zero
    if freq == 0 or freq_unigram[w1] == 0 or freq_unigram[w2] == 0:
        return 0

    p_w1_w2 = freq / N  # Joint probability
    p_w1 = freq_unigram[w1] / N  # Probability of w1
    p_w2 = freq_unigram[w2] / N  # Probability of w2

    return log(p_w1_w2 / (p_w1 * p_w2), 2)  # Base-2 log

# Apply the PMI calcualtion function
bigram_pmi = [
    (bigram, calculate_pmi(bigram, freq, freq_unigram, N))
    for bigram, freq in freq_bigrams.items()
]

# Take top 200 bigram
top_200_bigrams = sorted(bigram_pmi, key=lambda x: x[1], reverse=True)[:200]

print(top_200_bigrams)

"""Makes sure the collocations can be collocated within the same review."""

# Create a function that check the bigram appear in reviews
def check_collocations_in_reviews(top_bigrams, reviews):
    collocated_reviews = []
    for review in reviews:
        tokens = review.lower().split()  # Tokenize review
        matched_bigrams = [
            bigram for bigram, _ in top_bigrams
            if bigram in zip(tokens, tokens[1:])
        ]
        if matched_bigrams:
            collocated_reviews.append((review, matched_bigrams))
    return collocated_reviews

# Check the top 200 bigrams in reviews
collocated_reviews = check_collocations_in_reviews(top_200_bigrams, reviews)

print(collocated_reviews)

"""Once the unigrams and bigrams are obtained, merge them together."""

# Take top 200 bigrams without PMI
top_200_bigrams = [bigram for bigram, _ in sorted(bigram_pmi, key=lambda x: x[1], reverse=True)[:200]]

# Join bigram with underscore
top_200_bigrams_str = ['_'.join(bigram) for bigram in top_200_bigrams]

# Take unigrams with the frequencies
unigram_freq = freq_unigram.copy()

# Add bigrams with the frequencies into unigrams
for bigram in top_200_bigrams_str:
    unigram_freq[bigram] = freq_bigrams[tuple(bigram.split('_'))]

# Sort vocab based on alphabet
sorted_vocab = sorted(unigram_freq.items())

print(sorted_vocab)

"""-------------------------------------

<div class="alert alert-block alert-warning">
    
### 4.3. Generate numerical representation<a class="anchor" name="bigrams"></a>

One of the tasks is to generate the numerical representation for all tokens in abstract. This representation will facilitate further analysis by providing a structured format for evaluating token frequencies and relationships within the text.
"""

# Mapping token to index
token_to_index = {token: i for i, (token, _) in enumerate(sorted_vocab)}

# Function to extact gmap_id
def generate_gmap_id():
    return uuid.uuid4().hex

# Function to calculate token frequency in every review and make sparse format
def generate_sparse_representation(unigram_tokens):
    sparse_representation = []

    for review in unigram_tokens:
        freq = defaultdict(int)
        for token in review:
            if token in token_to_index:
                index = token_to_index[token]
                freq[index] += 1

        # Create string with required format
        gmap_id = generate_gmap_id()
        tokens_str = ",".join(f"{idx}:{count}" for idx, count in sorted(freq.items()))
        sparse_representation.append(f"{gmap_id}:{tokens_str}")

    return sparse_representation

# Generate sparse representation
sparse_data = generate_sparse_representation(unigram_tokens)

print(sparse_data)

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 5. Writing Output Files <a class="anchor" name="write"></a>

The files that need to be generated:
* Vocabulary list
* Sparse matrix (count_vectors)

This is performed in the following sections.

-------------------------------------

<div class="alert alert-block alert-warning">
    
### 5.1. Vocabulary List <a class="anchor" name="write-vocab"></a>

List of vocabulary should also be written to a file, sorted alphabetically, with their reference codes in front of them. This file also refers to the sparse matrix in the next file. The purpose of this is to ensure consistency between the text processing stage and the modeling stage, where the text data is represented numerically for further analysis.
"""

# Save the vocab and frequency into 008_vocab.txt
with open(os.path.join(student_008, "008_vocab.txt"), 'w') as f:
    for word, freq in sorted_vocab:
        f.write(f"{word}:{freq}\n")

"""-------------------------------------

<div class="alert alert-block alert-warning">
    
### 5.2. Sparse Matrix <a class="anchor" name="write-sparseMat"></a>

For writing sparse matrix for a paper, we firstly calculate the frequency of words for that paper ....
"""

# Save the sparse matrix into 008_countvec.txt
with open(os.path.join(student_008, "008_countvec.txt"), "w") as f:
    f.write("\n".join(sparse_data))

"""-------------------------------------

<div class="alert alert-block alert-success">
    
## 6. Summary <a class="anchor" name="summary"></a>

The project involved processing text data to extract unigrams and top 200 bigrams, calculate their frequencies, and create a vocabulary list alongside a sparse numerical representation. Challenges included handling zero division errors and formatting outputs correctly, which were resolved through error handling and systematic formatting techniques to ensure efficient data processing without reloading frequency data.

-------------------------------------

<div class="alert alert-block alert-success">
    
## 7. References <a class="anchor" name="Ref"></a>

1.   Set Threshold, OpenAI Documentation, https://www.openai.com/docs/threshold, Accessed 20/10/2024.
2.   PMI Calculation, OpenAI Documentation, https://www.openai.com/docs/pmi, Accessed 22/10/2024.
3.   Collocation, OpenAI Documentation, https://www.openai.com/docs/collocation, Accessed 22/10/2024.
4.   UUID, OpenAI Documentation, https://www.openai.com/docs/uuid, Accessed 23/10/2024.
5.   Debugging Techniques, https://www.openai.com/chatgpt, Accessed 23/10/2024.

-------------------------------------
"""