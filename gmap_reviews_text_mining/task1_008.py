# -*- coding: utf-8 -*-
"""task1_008.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12GZYod69alDZF7bJNY1HiK8_LzAe05On

<div class="alert alert-block alert-success">
    
# FIT5196 Task 1 in Assessment 1
#### Student Name: Robiatul Adawiyah Al-Qosh
#### Student ID: 34269193

Date: 14/10/2024


Environment: Python 3.10

Libraries used:
* re (for regular expression, installed and imported)
* pandas (for data manipulation)
* os (for acces files in folder)
* datetime (for datetime converting)
* json (for creating JSON file)


    
</div>

<div class="alert alert-block alert-danger">
    
## Table of Contents

</div>    

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Patent Files](#examine) <br>
[4. Loading and Parsing Files](#load) <br>
$\;\;\;\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>
$\;\;\;\;$[4.2. Reading Files](#Read) <br>
$\;\;\;\;$[4.3. Whatever else](#latin) <br>
[5. Writing to CSV/JSON File](#write) <br>
$\;\;\;\;$[5.1. Verification - using the sample files](#test_xml) <br>
[6. Summary](#summary) <br>
[7. References](#Ref) <br>

-------------------------------------

<div class="alert alert-block alert-warning">

## 1.  Introduction  <a class="anchor" name="Intro"></a>
    
</div>

This assessment regards extracting data from semi-sctuctured text files. The dataset contained 15 txt files and 1 Excel file (contain 16 sheets) which included various information about user reviews. In particular, the txt files store attributes like user_id, gmap_id, date, rating, review_text, etc., while the Excel sheets provide metadata. The goal is to parse, clean, and merge the data by normalizing timestamps and ensuring consistent tag structures.

-------------------------------------

<div class="alert alert-block alert-warning">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>
 </div>

The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:

* **re:** to define and use regular expressions
* **pandas:** to do data manipulation
* **os:** to access all the necessary file at once
* **datetime:** to convert timestamp in time tag
* **json:** to create JSON file
"""

import re
import pandas as pd
import os
from datetime import datetime
import json

from google.colab import drive
drive.mount('/content/drive')

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 3.  Examining Raw Data <a class="anchor" name="examine"></a>

 </div>

First of all, set the working directory that contains all the source files.
"""

student_008 = "/content/drive/MyDrive/S2 Monash University/MUI Y2 T1/ITI5196 Data Wrangling/Assignment 1/student_008"
# Plese adjust the path

"""Look all the tags in the txt files.


"""

# Using regex to see all the tags
tag_pattern = re.compile(r"<(.*?)>") # Tags are inside the "<>"

# Create a function to extract the tags from the file
def extract_tags_from_file(filepath):
    with open(filepath, "r", encoding="utf-8") as file:
        content = file.read()
    tags = tag_pattern.findall(content)
    return tags

# Loop for all txt file inside the working directory
all_tags = set()  # Using set to avoid duplicate
for filename in os.listdir(student_008):
    if filename.endswith(".txt"):
        file_tags = extract_tags_from_file(os.path.join(student_008, filename))
        all_tags.update(file_tags)

# Show all tags
print("All Tags:")
print(sorted(all_tags))

"""Count the number of data that is recorded inside the txt files."""

# Create a function to count tags in a file
def count_tags_in_file(filepath, tags):
    counts = {tag: 0 for tag in tags}
    with open(filepath, "r", encoding="utf-8") as file:
        content = file.read()
    for tag in tags:
        pattern = re.compile(fr"<({tag})>")
        counts[tag] += len(pattern.findall(content))
    return counts

# Create a function that count the total tags from all tha file
def count_tags_in_all_files(tags, folder):
    total_counts = {tag: 0 for tag in tags}
    for filename in os.listdir(folder):
        if filename.endswith(".txt"):
            filepath = os.path.join(folder, filename)
            file_counts = count_tags_in_file(filepath, tags)
            for tag, count in file_counts.items():
                total_counts[tag] += count
    return total_counts

# Fill the tags with record that needed (the tags can be added with tag needed)
tags = ["record"]
total_counts = count_tags_in_all_files(tags, student_008)

# Show the total tags
print("\nTotal data recorded in txt files:")
for tag, total in total_counts.items():
    print(f"  <{tag}>: {total}")

"""Having examined the file content, the following observations were made:

*   There are inconsistencies in tags that used in txt files. So, the appropriate regex must be used due to include the intended tags.
*   The number of data that is recorded in the txt files is 33,782 based on the total of record tag.

-------------------------------------

<div class="alert alert-block alert-warning">

## 4.  Loading and Parsing Files <a class="anchor" name="load"></a>

</div>

In this section, the files are parsed and processed. First of all, appropriate regular expressions are defined to extract desired information when reading the files. The focus is on handling variations in tag structures and inconsistencies, such as differences in capitalization or spacing. Each file is read sequentially, and relevant attributes like gmap_id, user_id, time, rating, text, pics, and resp are extracted using the defined regex patterns. The extracted data is then stored in a structured list for further processing.

-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.1. Defining Regular Expressions <a class="anchor" name="Reg_Exp"></a>

Defining correct regular expressions is crucial in extracting desired information from the text efficiently. The regular expressions are designed to handle minor inconsistencies across the files, such as optional spaces, mixed-case tags, or multiple closing slashes. For example, patterns for \<user_id> and \<UserId> are unified using flexible regex patterns to ensure both formats are captured. Additionally, backreferences are used to ensure matching pairs of opening and closing tags, preventing extraction errors. The patterns are applied with case-insensitive matching to ensure robustness and accuracy in parsing.
"""

pattern_id = re.compile(  #reg ex pattern or id
    r"<\s*[Gg]map_?[Ii][Dd]\s*>(.*?)<.*\/*[Gg]map_?[Ii][Dd]\s*>|"           # gmap_id
    r"<\s*[Uu]ser_?I?i?d?\.?\s*>(.*?)<.*\/*[Uu]ser_?I?i?d?\.?\s*/*>|"       # user_id
    r"<\s*[DdTt][ai][tm]e\s*>\s*(\d+)?\s*<.*\/*[DdTt][ai][tm]e\s*>|"        # time
    r"<\s*[Rr]ate?[ing]*\s*>(.*?)<.*\/*[Rr]ate?[ing]*\s*>|"                 # rating
    r"<\s*[RrTt]e[vx][it][iew]*\s*>(.*?)<.*\/*[RrTt]e[vx][it][iew]*\s*>|"   # text (review)
    r"<\s*[Pp]ics?[tures]*\s*>(.*?)<.*\/*[Pp]ics?[tures]*\s*>|"             # pics
    r"<\s*[Rr]esp[onse]*\s*>(.*?)<.*\/*[Rr]esp[onse]*\s*\/?>",              # resp
    re.DOTALL)

"""These patterns are used in the next step when reading the files.

-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.2. Reading Files <a class="anchor" name="Read"></a>

In this step, all files are read and parsed. The first files to be handled are the txt files.
"""

# Create the blank list for the data form the txt files
txt_data = []

# Create a function to convert the timestamp in time data
def convert_timestamp(timestamp):
    if timestamp:  # Pastikan timestamp tidak kosong
        return datetime.utcfromtimestamp(int(timestamp) / 1000).strftime('%Y-%m-%d %H:%M:%S')
    return None

# Loop to process all the txt files in the working directory
for filename in os.listdir(student_008):
    if filename.endswith(".txt"):
        with open(os.path.join(student_008, filename), "r", encoding="utf-8") as file:
            content = file.read()

        # Split the data based on <record>...</record>
        records = re.split(r"<\s*record[^>]*>.*?", content, flags=re.DOTALL)

        # Extrac data in every record with previous regex pattern
        for record in records:
            matches = pattern_id.findall(record)

            if matches:
                # Create the blank dictionary for every necessary tags
                record_data = {
                    "gmap_id": None,
                    "user_id": None,
                    "time": None,
                    "rating": None,
                    "text": None,
                    "pics": None,
                    "resp": None
                }

                # Fill the data from files
                for match in matches:
                    gmap_id, user_id, time, rating, text, pics, resp = match
                    if gmap_id:
                        record_data["gmap_id"] = gmap_id.strip()
                    if user_id:
                        record_data["user_id"] = user_id.strip()
                    if time:
                        record_data["time"] = convert_timestamp(time)
                    if rating:
                        record_data["rating"] = int(rating)
                    if text:
                        record_data["text"] = text.strip()
                    if pics:
                        record_data["pics"] = pics.strip()
                    if resp:
                        record_data["resp"] = resp.strip()

                txt_data.append(record_data)

"""Let's take a look at the first ten elements of the lists generated. We can see that ids, reviews,etc. are parsed and stored correctly."""

# See the first 10 data from the txt files
txt_data[:10]

"""-------------------------------------

<div class="alert alert-block alert-info">
    
### 4.3. Whatever else <a class="anchor" name="latin"></a>

the rest of your methodology

Create a new data frame contians the data from txt files.
"""

# Chenge list to data frame
df_txt = pd.DataFrame(txt_data)

# Show the data
df_txt

"""After the txt data is done and in a data frame form, the Excel file can be handled."""

# Read the excel file from the working directory
excel_data = pd.read_excel(os.path.join(student_008, "group008.xlsx"), sheet_name = None)

# Join all the table in all sheet based on the header
df_excel = pd.concat(excel_data.values(), ignore_index = True)

# Show the data
df_excel

"""Remove the unnecessary columns such as x0, x1, etc. and the name column, since it is not needed and do not match the txt data and assignment required file. Then, remove the extra rows that contains NaN or blank in all the cells."""

# Drop all the unnecessary columns and the name column to fit the data frome txt files
df_excel = df_excel.drop(columns = ["x0", "x1", "x2", "x3", "x4", "name"])

# Delete the all NaN rows
df_excel = df_excel.dropna(how = "all")

# Show the data
df_excel

"""Change the data type in time column into proper date time."""

# Convert time column to date time data type
df_excel["time"] = pd.to_datetime(df_excel["time"], unit = "ms")

# Show the data
df_excel

"""Combine all the data both from txt files and excel file together in one data frame."""

# Join the txt data frame and excel data frame
df_all = pd.concat([df_txt, df_excel], ignore_index = True)

# Show the data
df_all

"""There are inconsistencies in blank/missing values, so standardization is carried out. The decision to keep missing values reamin blank instead of filling it in was made to maintain the authenticity of the data."""

# Change the None values from the txt data into <NA>
df_all = df_all.replace("None", pd.NA)

# Change the NaN values from the excel data into <NA>
df_all = df_all.astype("object").where(pd.notna(df_all), pd.NA)

df_all

"""Next, the information of the data need to be known to run another process."""

df_all.info()

"""Since the time and rating columns are not in appropriate data type, changes must be made. The time data will be converted into datetime data type, while the rating data will be converted into integer."""

# Convert time from object into datetime data type
df_all["time"] = pd.to_datetime(df_all["time"])

# Convert rating from object into int64
df_all["rating"] = pd.to_numeric(df_all["rating"], errors = "coerce").astype("Int64")

"""Look at the data information again."""

df_all.info()

"""To get another data information, the describe() command can be ran. It will shows data statistics and another information."""

df_all.describe()

# Describe for un-numeric columns
df_all.describe(include = ['O'])

"""Afterwhich, the data duplicate need to be removed to avoid data inaccuracy."""

# Drop all the duplicate from the data
df_all = df_all.drop_duplicates(ignore_index = True)

df_all

"""The next step is make sure that the review text should be transformed into lower case with no emojis."""

# Change the text into lower case
df_all["text"] = df_all["text"].str.lower()

df_all

"""Defining regular expression for the emojis in utf-8."""

emoji_pattern = re.compile(
    "["
    "\U0001F600-\U0002F64F"
    "\U0001F300-\U0001F5FF"
    "\U0001F680-\U0001F6FF"
    "\U0001F1E0-\U0001F1FF"
    "\U00002702-\U000027B0"
    "\U000024C2-\U0001F251"

    # Additional emoji ranges not covered in the original pattern
    "\U0001F700-\U0001F77F"  # Alchemical symbols
    "\U0001F780-\U0001F7FF"  # Geometric shapes (extended)
    "\U0001F800-\U0001F8FF"  # Supplemental arrows and symbols
    "\U0001F900-\U0001F9FF"  # Supplemental symbols and pictographs
    "\U0001FA00-\U0001FA6F"  # Symbols for tools, animals, and household objects
    "\U0001FA70-\U0001FAFF"  # Symbols for accessibility and activities
    "\U00002600-\U000026FF"  # Miscellaneous symbols
    "\U00002B50-\U00002B59"  # Star symbols
    "]+"
)

# Delete the emojis from the text
df_all["text"] = df_all["text"].apply(lambda x: emoji_pattern.sub(r'', x) if isinstance(x, str) else x)

df_all

df_all.shape

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 5.  Writing to Output Files <a class="anchor" name="write"></a>

</div>

The assignment the output files required are CSV file and JSON file. The CSV file contains summary from the data, whilw JSON file contains the detail about the data.

-------------------------------------

<div class="alert alert-block alert-info">
    
### 5.1. Verification of the Generated CSV File <a class="anchor" name="test_xml"></a>

First of all, a new data frame containing the gmap_id, review_count, review_text_count, and response_count need to be created.
"""

# Create the csv data frame
df_csv = df_all.groupby('gmap_id').agg(
    review_count=('gmap_id', 'size'),  # Count the number of review per gmap_id
    review_text_count=('text', lambda x: x.notna().sum()),  # Count the review text
    response_count=('resp', lambda x: x.notna().sum())  # Count the response
).reset_index() # Create index

df_csv

"""Then, save the data frame as a csv file in the student_008 working directory."""

# Save data frame as csv file
df_csv.to_csv(os.path.join(student_008, 'task1_008.csv'), index=False)

print(f"Successfully saved task1_008.csv")

"""-------------------------------------

<div class="alert alert-block alert-info">
    
### 5.2. Verification of the Generated JSON File <a class="anchor" name="test_xml"></a>

Before making the JSON file, the important functions must be created first.
"""

# Function to decide there is pic or not in the review
def if_pics(pics):
     return 'Y' if pd.notna(pics) and bool(pics) else 'N'

# Function to exctract the dimension of the pics
def extract_dimensions(pics):
    if pd.isna(pics):
        return [] # Empty list if blank

    pics = str(pics) # Make sure the pics are string
    matches = re.findall(r'=w(\d+)-h(\d+)', pics) # Find pattern =wXXX-hXXX with regex
    dimensions = [[width, height] for width, height in matches]

    return dimensions

# Function to decide there is resp or not in the review
def if_resp(resp):
    return 'Y' if pd.notna(resp) and bool(resp) else 'N'

# Funtion to change pd.NA with 'None'
def replace_na_with_none(data):
    if isinstance(data, list):
        return [replace_na_with_none(item) for item in data]
    elif isinstance(data, dict):
        return {key: replace_na_with_none(value) for key, value in data.items()}
    elif pd.isna(data):
        return 'None'
    else:
        return data

# Group review based on gmap_id
grouped = df_all.groupby('gmap_id')

# Make blank list for output
output = []

for gmap_id, group in grouped:
    reviews = []
    for _, row in group.iterrows():
        # Review structure
        review = {
            'user_id': row.get('user_id'),
            'time': row['time'].strftime('%Y-%m-%d %H:%M:%S'),
            'review_rating': row.get('rating'),
            'review_text': row.get('text'),
            'if_pic': if_pics(row.get('pics')),
            'pic_dim': extract_dimensions(row.get('pics')),
            'if_response': if_resp(row.get('resp')),
        }
        reviews.append(review)

    # Earliest and latest date
    earliest_review_date = group['time'].min().strftime('%Y-%m-%d %H:%M:%S')
    latest_review_date = group['time'].max().strftime('%Y-%m-%d %H:%M:%S')

    # JSON structure for every business (gmap_id)
    business_data = {
        'gmap_id': gmap_id,
        'reviews': reviews,
        'earliest_review_date': earliest_review_date,
        'latest_review_date': latest_review_date
    }

    # Append into the output
    output.append(business_data)

# Change null to None in the output
output_cleaned = replace_na_with_none(output)

"""Then, save the data output as a JSON file in the same student_008 working directory."""

# Save the output as JSON file
with open(os.path.join(student_008, 'task1_008.json'), 'w', encoding='utf-8') as f:
    json.dump(output_cleaned, f, indent=2, ensure_ascii=False)

print(f"Successfully saved task1_008.json")

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 6. Summary <a class="anchor" name="summary"></a>

</div>

This code efficiently parses Google Map review data from semi-structured text files and an Excel file, extracting key attributes such as user ID, review time, rating, text, and picture dimensions. A notable challenge was handling varied formats of picture URLs and ensuring correct data types, which was addressed by using regular expressions for accurate parsing and normalization. The output is structured in both CSV and JSON formats, adhering to specified requirements while maintaining data integrity through emoji removal and lowercase transformation. Overall, the implementation demonstrates robust data processing techniques in Python.

-------------------------------------

<div class="alert alert-block alert-warning">

## 7. References <a class="anchor" name="Ref"></a>

</div>

1.   <a class="anchor" name="ref-2"></a>  Additional Emojis in Regex, https://www.openai.com/chatgpt, Accessed 19/10/2024.
2.   <a class="anchor" name="ref-3"></a>  Managing Index Order in DataFrames, https://www.openai.com/chatgpt, Accessed 20/10/2024.
3.   <a class="anchor" name="ref-2"></a>  Debugging Techniques, https://www.openai.com/chatgpt, Accessed 20/10/2024.

-------------------------------------
"""